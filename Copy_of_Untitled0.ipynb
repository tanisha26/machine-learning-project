{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICxEcX0bXNBe"
      },
      "outputs": [],
      "source": [
        " !pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "M9sqMrG_Xr_i"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark in Google Colab\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "wkEcelqmXxvd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "G4DPw0lEX82x"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt the user to upload a file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "fp5ABejqYrBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the uploaded file name\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Uploaded file: {filename}\")"
      ],
      "metadata": {
        "id": "nE9tchcjdIgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CarInsurancePrediction\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the car insurance dataset into a DataFrame\n",
        "final_numeric_car_df = spark.read.csv(\"final_numeric_car_df.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "vKTvSRQbfM3l"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CarInsurancePrediction\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the car insurance dataset\n",
        "final_numeric_car_df = spark.read.csv(\"final_numeric_car_df.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random_seed = 1999\n",
        "\n",
        "# Calculate the number of rows in the car insurance dataset\n",
        "n_rows = final_numeric_car_df.count()\n",
        "\n",
        "# Sample 70% of indices for training\n",
        "training_idx = final_numeric_car_df.sample(False, 0.7, seed=random_seed)\n",
        "\n",
        "# Create training and test datasets\n",
        "training_car_insurance = final_numeric_car_df.subtract(training_idx)\n",
        "test_car_insurance = training_idx\n",
        "\n",
        "# Define the features vector\n",
        "feature_cols = ['Kids_Drive', 'Age', 'Kids_Home', 'Year_at_Job', 'Income', 'Single_Parent', 'Home_Val', 'Marital_Status', 'Gender', 'Education', 'Occupation', 'Travel_Time', 'Car_Use', 'Car_Value', 'Time_in_Force', 'Car_Type', 'Old_Claim', 'Claim_Freq', 'License_Revoked', 'Vehicle_Record_Points', 'Car_Age', 'Urbanicity']\n",
        "\n",
        "# Create the VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Transform the DataFrame to include the features vector\n",
        "training_car_insurance = assembler.transform(training_car_insurance)\n",
        "test_car_insurance = assembler.transform(test_car_insurance)\n",
        "\n",
        "# Rename the label column to 'label'\n",
        "training_car_insurance = training_car_insurance.withColumnRenamed(\"Claim_Flag\", \"label\")\n",
        "\n",
        "# Define the layers for the neural network\n",
        "input_layer_size = len(feature_cols)\n",
        "output_layer_size = 2  # Assuming binary classification\n",
        "layers = [input_layer_size, 10, output_layer_size]  # Example: input layer size, hidden layer size, output layer size\n",
        "\n",
        "# Create the MultilayerPerceptronClassifier model\n",
        "nn_model = MultilayerPerceptronClassifier(layers=layers, seed=1234)\n",
        "\n",
        "# Train the model and measure training time\n",
        "start_time = time.time()\n",
        "trained_model = nn_model.fit(training_car_insurance)\n",
        "training_time = time.time() - start_time\n",
        "print(\"Training Time:\", training_time)\n",
        "\n",
        "# Make predictions on the test dataset and measure prediction time\n",
        "start_prediction_time = time.time()\n",
        "predictions = trained_model.transform(test_car_insurance)\n",
        "prediction_time = time.time() - start_prediction_time\n",
        "print(\"Prediction Time:\", prediction_time)\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "# Compute Accuracy\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"Claim_Flag\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compute AUC\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"Claim_Flag\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator_auc.evaluate(predictions)\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "\n",
        "# Compute Sensitivity and Specificity\n",
        "TP = predictions.filter(\"prediction = 1 AND Claim_Flag = 1\").count()\n",
        "FP = predictions.filter(\"prediction = 1 AND Claim_Flag = 0\").count()\n",
        "TN = predictions.filter(\"prediction = 0 AND Claim_Flag = 0\").count()\n",
        "FN = predictions.filter(\"prediction = 0 AND Claim_Flag = 1\").count()\n",
        "\n",
        "# Compute Sensitivity\n",
        "sensitivity = TP / (TP + FN)\n",
        "\n",
        "# Compute Specificity\n",
        "specificity = TN / (TN + FP)\n",
        "\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "print(\"Specificity:\", specificity)\n",
        "\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3t3r3eyGgQIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Neural Network Evaluation Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the car insurance dataset\n",
        "final_numeric_car_df = spark.read.csv(\"final_numeric_car_df.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Check if the label column exists in the dataset\n",
        "if \"Claim_Flag\" not in final_numeric_car_df.columns:\n",
        "    raise ValueError(\"Label column 'Claim_Flag' not found in the dataset.\")\n",
        "\n",
        "# Check if the feature columns exist in the dataset\n",
        "feature_cols = [col for col in final_numeric_car_df.columns if col != \"Claim_Flag\"]\n",
        "if not feature_cols:\n",
        "    raise ValueError(\"No feature columns found in the dataset.\")\n",
        "\n",
        "# Define the feature assembler\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "final_numeric_car_df = assembler.transform(final_numeric_car_df)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "(training_data, test_data) = final_numeric_car_df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Define the neural network classifier\n",
        "nn_classifier = MultilayerPerceptronClassifier(layers=[len(feature_cols), 5, 2], seed=42, labelCol=\"Claim_Flag\")\n",
        "\n",
        "# Define the evaluator for accuracy calculation\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Claim_Flag\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Define the parameter grid for tuning\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(nn_classifier.maxIter, [10, 50]) \\\n",
        "    .addGrid(nn_classifier.stepSize, [0.01, 0.1]) \\\n",
        "    .build()\n",
        "\n",
        "# Define cross-validation\n",
        "crossval = CrossValidator(estimator=nn_classifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3)\n",
        "\n",
        "# Train and tune the model\n",
        "start_time = time.time()\n",
        "cv_model = crossval.fit(training_data)\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate training time\n",
        "training_time = end_time - start_time\n",
        "print(\"Training Time:\", training_time)\n",
        "\n",
        "# Get the best model from cross-validation\n",
        "best_model = cv_model.bestModel\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = best_model.transform(test_data)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Get the best model's parameters\n",
        "best_max_iter = best_model.getMaxIter()\n",
        "best_step_size = best_model.getStepSize()\n",
        "print(\"Best Max Iteration:\", best_max_iter)\n",
        "print(\"Best Step Size:\", best_step_size)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "eQMVbqgYIYxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Neural Network Evaluation Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the car insurance dataset\n",
        "final_numeric_car_df = spark.read.csv(\"final_numeric_car_df.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Check if the label column exists in the dataset\n",
        "if \"Claim_Flag\" not in final_numeric_car_df.columns:\n",
        "    raise ValueError(\"Label column 'Claim_Flag' not found in the dataset.\")\n",
        "\n",
        "# Check if the feature columns exist in the dataset\n",
        "feature_cols = [col for col in final_numeric_car_df.columns if col != \"Claim_Flag\"]\n",
        "if not feature_cols:\n",
        "    raise ValueError(\"No feature columns found in the dataset.\")\n",
        "\n",
        "# Define the feature assembler\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "final_numeric_car_df = assembler.transform(final_numeric_car_df)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "(training_data, test_data) = final_numeric_car_df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Define the neural network classifier\n",
        "nn_classifier = MultilayerPerceptronClassifier(layers=[len(feature_cols), 5, 2], seed=42, labelCol=\"Claim_Flag\")\n",
        "\n",
        "# Train the neural network model\n",
        "start_time = time.time()\n",
        "nn_model = nn_classifier.fit(training_data)\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate training time\n",
        "training_time = end_time - start_time\n",
        "print(\"Training Time:\", training_time)\n",
        "\n",
        "# Make predictions on the test data\n",
        "start_prediction_time = time.time()\n",
        "predictions = nn_model.transform(test_data)\n",
        "end_prediction_time = time.time()\n",
        "\n",
        "# Calculate prediction time\n",
        "prediction_time = end_prediction_time - start_prediction_time\n",
        "print(\"Prediction Time:\", prediction_time)\n",
        "\n",
        "# Calculate AUC\n",
        "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"Claim_Flag\")\n",
        "auc = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "\n",
        "# Calculate sensitivity and specificity\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "print(\"Specificity:\", specificity)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "ZeJtaVRBJKQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}